

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>图像数据评估指标 &mdash; Open-DataFlow-Eval 0.1 文档</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />

  
      <script src="../../_static/jquery.js?v=5d32c60e"></script>
      <script src="../../_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="../../_static/documentation_options.js?v=7c91f8fd"></script>
      <script src="../../_static/doctools.js?v=9bcbadda"></script>
      <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
      <script src="../../_static/translations.js?v=beaddf03"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="索引" href="../../genindex.html" />
    <link rel="search" title="搜索" href="../../search.html" />
    <link rel="next" title="Text Data Evaluation Metrics" href="text_metrics.html" />
    <link rel="prev" title="Image Data Evaluation Metrics" href="image_metrics.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Open-DataFlow-Eval
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <p class="caption" role="heading"><span class="caption-text">Open-DataFlow-Eval:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../getting_started/index.html">Getting Started</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Metrics</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="image_metrics.html">Image Data Evaluation Metrics</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">图像数据评估指标</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id2">纯图像评估指标</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id3">指标分类</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id4">针对真实图像的评估指标</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id7">针对生成图像的评估指标</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id9">图像-文本评估指标</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id10">图文对齐指标</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sft">图文SFT数据评估指标</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="text_metrics.html">Text Data Evaluation Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="text_metrics.zh-CN.html">文本数据评估指标</a></li>
<li class="toctree-l2"><a class="reference internal" href="video_metrics.html">Video Data Evaluation Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="video_metrics.zh-CN.html">视频数据评估指标</a></li>
<li class="toctree-l2"><a class="reference internal" href="synth_metrics.html">Synth</a></li>
</ul>
</li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Open-DataFlow-Eval</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="index.html">Metrics</a></li>
      <li class="breadcrumb-item active">图像数据评估指标</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/src/metrics/image_metrics.zh-CN.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="id1">
<h1>图像数据评估指标<a class="headerlink" href="#id1" title="Link to this heading"></a></h1>
<!-- 使用`dataflow.list_image_eval_metrics()`打印所有可用的图像评估指标。
```python
import dataflow
dataflow.list_image_eval_metrics()
``` -->
<section id="id2">
<h2>纯图像评估指标<a class="headerlink" href="#id2" title="Link to this heading"></a></h2>
<section id="id3">
<h3>指标分类<a class="headerlink" href="#id3" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>类别描述</p></th>
<th class="head"><p>指标列表</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>基于图像统计信息</p></td>
<td><p>BRISQUE、ILNIQE、NIQE、PIQE、FID、KID、IS</p></td>
</tr>
<tr class="row-odd"><td><p>基于神经网络</p></td>
<td><p>ARNIQA、TOPIQ、TReS、MANIQA、MUSIQ、DBCNN、PaQ-2-PiQ、HyperIQA、NIMA、WaDIQaM、CNNIQA</p></td>
</tr>
<tr class="row-even"><td><p>基于预训练图像-文本模型</p></td>
<td><p>Q-Align、CLIPIQA(+)、 LIQE</p></td>
</tr>
</tbody>
</table>
</section>
<section id="id4">
<h3>针对真实图像的评估指标<a class="headerlink" href="#id4" title="Link to this heading"></a></h3>
<section id="id5">
<h4>指标介绍<a class="headerlink" href="#id5" title="Link to this heading"></a></h4>
<p>本仓库调用<a class="reference external" href="https://github.com/chaofengc/IQA-PyTorch">pyiqa</a>包中的non-reference（NR）算法进行纯图像数据质量评估，各评估指标的介绍可参考<a class="reference external" href="https://github.com/chaofengc/IQA-PyTorch/blob/main/docs/ModelCard.md">Py-IQA Model Card</a>。</p>
<p>说明：当同一指标使用了不同训练数据集时，我们使用<code class="docutils literal notranslate"><span class="pre">指标名-数据集名</span></code>进行区分。比如，<code class="docutils literal notranslate"><span class="pre">arniqa-csiq</span></code>中的<code class="docutils literal notranslate"><span class="pre">csiq</span></code>即为数据集名称。当没有标注数据集名时，默认为<code class="docutils literal notranslate"><span class="pre">koniq</span></code>，比如，<code class="docutils literal notranslate"><span class="pre">arniqa</span></code>对应的数据集为<code class="docutils literal notranslate"><span class="pre">koniq</span></code>。</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>指标</p></th>
<th class="head"><p>名称（用于<code class="docutils literal notranslate"><span class="pre">datagym.get_scorer()</span></code>）</p></th>
<th class="head"><p>评估维度</p></th>
<th class="head"><p>简介</p></th>
<th class="head"><p>取值范围</p></th>
<th class="head"><p>官方仓库或论文</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Q-Align</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">qalign</span></code> (with quality[default], aesthetic options)</p></td>
<td><p>基于预训练图像-文本模型</p></td>
<td><p>使用视觉LLM进行打分。得分越高代表图像质量越高。</p></td>
<td><p>[1,5]</p></td>
<td><p><a class="reference external" href="https://github.com/Q-Future/Q-Align">code</a></p></td>
</tr>
<tr class="row-odd"><td><p>LIQE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">liqe</span></code>, <code class="docutils literal notranslate"><span class="pre">liqe_mix</span></code></p></td>
<td><p>基于预训练图像-文本模型</p></td>
<td><p>基于CLIP。得分越高代表图像质量越高。</p></td>
<td><p>[1,5]</p></td>
<td><p><a class="reference external" href="https://github.com/zwx8981/LIQE">code</a></p></td>
</tr>
<tr class="row-even"><td><p>ARNIQA</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">arniqa</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-live</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-csiq</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-tid</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-kadid</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-clive</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-flive</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-spaq</span></code></p></td>
<td><p>基于神经网络</p></td>
<td><p>学习图像失真流形。得分越高代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2310.14918">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>TOPIQ</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">topiq_nr</span></code>, <code class="docutils literal notranslate"><span class="pre">topiq_nr-flive</span></code>, <code class="docutils literal notranslate"><span class="pre">topiq_nr-spaq</span></code></p></td>
<td><p>基于神经网络</p></td>
<td><p>基于语义的自顶向下图像质量评估。得分越高代表图像质量越高。</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2308.03060">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>TReS</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">tres</span></code>, <code class="docutils literal notranslate"><span class="pre">tres-flive</span></code></p></td>
<td><p>基于神经网络</p></td>
<td><p>通过相对排名和自我一致性增强指标的鲁棒性。得分越高代表图像质量越高。</p></td>
<td><p>[0,100]</p></td>
<td><p><a class="reference external" href="https://github.com/isalirezag/TReS">code</a></p></td>
</tr>
<tr class="row-odd"><td><p>CLIPIQA(+)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">clipiqa</span></code>, <code class="docutils literal notranslate"><span class="pre">clipiqa+</span></code>, <code class="docutils literal notranslate"><span class="pre">clipiqa+_vitL14_512</span></code>,<code class="docutils literal notranslate"><span class="pre">clipiqa+_rn50_512</span></code></p></td>
<td><p>基于预训练图像-文本模型</p></td>
<td><p>基于CLIP设计Antonym prompt pairing（反义提示词对）。使用了不同骨干网络的CLIPIQA(+)，默认为RN50。得分越高代表图像质量越高。</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://github.com/IceClear/CLIP-IQA">code</a></p></td>
</tr>
<tr class="row-even"><td><p>MANIQA</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">maniqa</span></code>, <code class="docutils literal notranslate"><span class="pre">maniqa-kadid</span></code>, <code class="docutils literal notranslate"><span class="pre">maniqa-pipal</span></code></p></td>
<td><p>基于神经网络</p></td>
<td><p>设计了多维注意力网络用于质量评估。得分越高代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2204.08958">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>MUSIQ</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">musiq</span></code>, <code class="docutils literal notranslate"><span class="pre">musiq-spaq</span></code>, <code class="docutils literal notranslate"><span class="pre">musiq-paq2piq</span></code>, <code class="docutils literal notranslate"><span class="pre">musiq-ava</span></code></p></td>
<td><p>基于神经网络</p></td>
<td><p>设计了多尺度图像质量评估Transformer。得分越高代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2108.05997">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>DBCNN</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">dbcnn</span></code></p></td>
<td><p>基于神经网络</p></td>
<td><p>设计了双线性模型来处理合成失真和真实失真。得分越高代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/document/8576582">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>PaQ-2-PiQ</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">paq2piq</span></code></p></td>
<td><p>基于神经网络</p></td>
<td><p>设计了产生全局到局部推断以及局部到全局推断的质量评估结构。得分越高代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://baidut.github.io/PaQ-2-PiQ/">code</a></p></td>
</tr>
<tr class="row-even"><td><p>HyperIQA</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">hyperiqa</span></code></p></td>
<td><p>基于神经网络</p></td>
<td><p>设计了自适应超网络架构以处理真实世界的图像失真。得分越高代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Su_Blindly_Assess_Image_Quality_in_the_Wild_Guided_by_a_CVPR_2020_paper.pdf">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>NIMA</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nima</span></code>, <code class="docutils literal notranslate"><span class="pre">nima-vgg16-ava</span></code></p></td>
<td><p>基于神经网络</p></td>
<td><p>使用卷积神经网络预测人类意见得分的<strong>分布</strong>。。得分越高代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1709.05424">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>WaDIQaM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">wadiqam_nr</span></code></p></td>
<td><p>基于神经网络</p></td>
<td><p>基于卷积神经网络。得分越高代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/8063957">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>CNNIQA</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cnniqa</span></code></p></td>
<td><p>基于神经网络</p></td>
<td><p>基于卷积神经网络。得分越高代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Kang_Convolutional_Neural_Networks_2014_CVPR_paper.pdf">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>NRQM(Ma)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nrqm</span></code></p></td>
<td><p>超分辨率图像评估</p></td>
<td><p>基于图像统计信息</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1612.05890">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>PI(Perceptual Index)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">pi</span></code></p></td>
<td><p>超分辨率图像评估</p></td>
<td><p>基于Ma's score和NIQE。得分越低代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1711.06077">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>BRISQUE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">brisque</span></code>, <code class="docutils literal notranslate"><span class="pre">brisque_matlab</span></code></p></td>
<td><p>基于图像统计信息</p></td>
<td><p>在空间域中进进行评估；计算复杂度低。得分越低代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/document/6272356">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>ILNIQE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ilniqe</span></code></p></td>
<td><p>基于图像统计信息</p></td>
<td><p>基于自然图像统计特征。得分越低代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/document/7094273">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>NIQE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">niqe</span></code>, <code class="docutils literal notranslate"><span class="pre">niqe_matlab</span></code></p></td>
<td><p>基于图像统计信息</p></td>
<td><p>基于自然、未失真的图像数据的统计特征。得分越低代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/document/6353522">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>PIQE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">piqe</span></code></p></td>
<td><p>基于图像统计信息</p></td>
<td><p>提取局部特征来预测质量；计算复杂度低。得分越低代表图像质量越高。</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/document/7084843">paper</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="id6">
<h4>参考值<a class="headerlink" href="#id6" title="Link to this heading"></a></h4>
<p>为更好的提供数据质量参考，我们使用以上指标对MSCOCO 2017 train进行评估，得到的指标数值分布如下:</p>
<!--<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-0pky{border-color:inherit;text-align:left;vertical-align:top}
</style> -->
<table class="tg"><thead>
  <tr>
    <th class="tg-0pky">指标</th>
    <th class="tg-0pky">名称</th>
    <th class="tg-0pky">均值</th>
    <th class="tg-0pky">方差</th>
    <th class="tg-0pky">最大值</th>
    <th class="tg-0pky">最小值</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-0pky">Q-Align</td>
    <td class="tg-0pky">qalign</td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="2">LIQE</td>
    <td class="tg-0pky">liqe</td>
    <td class="tg-0pky">4.152</td>
    <td class="tg-0pky">1.004</td>
    <td class="tg-0pky">5.000</td>
    <td class="tg-0pky">1.000</td>
  </tr>
  <tr>
    <td class="tg-0pky">liqe_mix</td>
    <td class="tg-0pky">4.090</td>
    <td class="tg-0pky">0.893</td>
    <td class="tg-0pky">5.000</td>
    <td class="tg-0pky">1.000</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="9">ARNIQA</td>
    <td class="tg-0pky">arniqa</td>
    <td class="tg-0pky">0.705</td>
    <td class="tg-0pky">0.069</td>
    <td class="tg-0pky">0.867</td>
    <td class="tg-0pky">0.150</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-clive</td>
    <td class="tg-0pky">0.649</td>
    <td class="tg-0pky">0.103</td>
    <td class="tg-0pky">0.961</td>
    <td class="tg-0pky">-0.105</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-csiq</td>
    <td class="tg-0pky">0.900</td>
    <td class="tg-0pky">0.073</td>
    <td class="tg-0pky">1.081</td>
    <td class="tg-0pky">0.319</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-flive</td>
    <td class="tg-0pky">0.724</td>
    <td class="tg-0pky">0.036</td>
    <td class="tg-0pky">0.838</td>
    <td class="tg-0pky">0.097</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-kadid</td>
    <td class="tg-0pky">0.635</td>
    <td class="tg-0pky">0.122</td>
    <td class="tg-0pky">0.965</td>
    <td class="tg-0pky">-0.013</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-koniq</td>
    <td class="tg-0pky">0.705</td>
    <td class="tg-0pky">0.069</td>
    <td class="tg-0pky">0.867</td>
    <td class="tg-0pky">0.150</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-live</td>
    <td class="tg-0pky">0.788</td>
    <td class="tg-0pky">0.069</td>
    <td class="tg-0pky">0.958</td>
    <td class="tg-0pky">0.227</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-spqa</td>
    <td class="tg-0pky">0.699</td>
    <td class="tg-0pky">0.104</td>
    <td class="tg-0pky">1.100</td>
    <td class="tg-0pky">0.056</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-tid</td>
    <td class="tg-0pky">0.548</td>
    <td class="tg-0pky">0.081</td>
    <td class="tg-0pky">0.803</td>
    <td class="tg-0pky">0.140</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="5">TOPIQ</td>
    <td class="tg-0pky">topiq_nr</td>
    <td class="tg-0pky">0.610</td>
    <td class="tg-0pky">0.116</td>
    <td class="tg-0pky">0.851</td>
    <td class="tg-0pky">0.073</td>
  </tr>
  <tr>
    <td class="tg-0pky">topiq_iaa_res50</td>
    <td class="tg-0pky">5.013</td>
    <td class="tg-0pky">0.492</td>
    <td class="tg-0pky">6.969</td>
    <td class="tg-0pky">2.812</td>
  </tr>
  <tr>
    <td class="tg-0pky">topiq_iaa</td>
    <td class="tg-0pky">4.838</td>
    <td class="tg-0pky">0.539</td>
    <td class="tg-0pky">7.129</td>
    <td class="tg-0pky">2.607</td>
  </tr>
  <tr>
    <td class="tg-0pky">topiq_nr-flive</td>
    <td class="tg-0pky">0.728</td>
    <td class="tg-0pky">0.036</td>
    <td class="tg-0pky">0.825</td>
    <td class="tg-0pky">0.371</td>
  </tr>
  <tr>
    <td class="tg-0pky">topiq_nr-spaq</td>
    <td class="tg-0pky">0.679</td>
    <td class="tg-0pky">0.102</td>
    <td class="tg-0pky">0.930</td>
    <td class="tg-0pky">0.119</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="4">CLIPIQA(+)</td>
    <td class="tg-0pky">clipiqa</td>
    <td class="tg-0pky">0.622</td>
    <td class="tg-0pky">0.149</td>
    <td class="tg-0pky">0.934</td>
    <td class="tg-0pky">0.056</td>
  </tr>
  <tr>
    <td class="tg-0pky">clipiqa+</td>
    <td class="tg-0pky">0.659</td>
    <td class="tg-0pky">0.100</td>
    <td class="tg-0pky">0.918</td>
    <td class="tg-0pky">0.130</td>
  </tr>
  <tr>
    <td class="tg-0pky">clipiqa+_rn50_512</td>
    <td class="tg-0pky">0.571</td>
    <td class="tg-0pky">0.122</td>
    <td class="tg-0pky">0.883</td>
    <td class="tg-0pky">0.050</td>
  </tr>
  <tr>
    <td class="tg-0pky">clipiqa+_vitL14_512</td>
    <td class="tg-0pky">0.593</td>
    <td class="tg-0pky">0.128</td>
    <td class="tg-0pky">0.893</td>
    <td class="tg-0pky">0.077</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="4">MANIQA</td>
    <td class="tg-0pky">maniqa</td>
    <td class="tg-0pky">0.454</td>
    <td class="tg-0pky">0.106</td>
    <td class="tg-0pky">0.789</td>
    <td class="tg-0pky">0.021</td>
  </tr>
  <tr>
    <td class="tg-0pky">maniqa-kadid</td>
    <td class="tg-0pky">0.637</td>
    <td class="tg-0pky">0.122</td>
    <td class="tg-0pky">0.877</td>
    <td class="tg-0pky">0.075</td>
  </tr>
  <tr>
    <td class="tg-0pky">maniqa-koniq</td>
    <td class="tg-0pky">0.454</td>
    <td class="tg-0pky">0.106</td>
    <td class="tg-0pky">0.789</td>
    <td class="tg-0pky">0.021</td>
  </tr>
  <tr>
    <td class="tg-0pky">maniqa-pipal</td>
    <td class="tg-0pky">0.676</td>
    <td class="tg-0pky">0.062</td>
    <td class="tg-0pky">0.888</td>
    <td class="tg-0pky">0.228</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="5">MUSIQ</td>
    <td class="tg-0pky">musiq</td>
    <td class="tg-0pky">69.086</td>
    <td class="tg-0pky">7.833</td>
    <td class="tg-0pky">79.559</td>
    <td class="tg-0pky">12.679</td>
  </tr>
  <tr>
    <td class="tg-0pky">musiq-ava</td>
    <td class="tg-0pky">4.939</td>
    <td class="tg-0pky">0.546</td>
    <td class="tg-0pky">7.269</td>
    <td class="tg-0pky">2.434</td>
  </tr>
  <tr>
    <td class="tg-0pky">musiq-koniq</td>
    <td class="tg-0pky">69.086</td>
    <td class="tg-0pky">7.833</td>
    <td class="tg-0pky">79.559</td>
    <td class="tg-0pky">12.679</td>
  </tr>
  <tr>
    <td class="tg-0pky">musiq-paq2piq</td>
    <td class="tg-0pky">72.792</td>
    <td class="tg-0pky">3.520</td>
    <td class="tg-0pky">79.772</td>
    <td class="tg-0pky">39.551</td>
  </tr>
  <tr>
    <td class="tg-0pky">musiq-spaq</td>
    <td class="tg-0pky">70.534</td>
    <td class="tg-0pky">8.661</td>
    <td class="tg-0pky">81.385</td>
    <td class="tg-0pky">14.290</td>
  </tr>
  <tr>
    <td class="tg-0pky">DBCNN</td>
    <td class="tg-0pky">dbcnn</td>
    <td class="tg-0pky">0.634</td>
    <td class="tg-0pky">0.100</td>
    <td class="tg-0pky">0.834</td>
    <td class="tg-0pky">0.143</td>
  </tr>
  <tr>
    <td class="tg-0pky">PaQ-2-PiQ</td>
    <td class="tg-0pky">paq2piq</td>
    <td class="tg-0pky">74.669</td>
    <td class="tg-0pky">3.731</td>
    <td class="tg-0pky">85.906</td>
    <td class="tg-0pky">15.859</td>
  </tr>
  <tr>
    <td class="tg-0pky">HyperIQA</td>
    <td class="tg-0pky">hyperiqa</td>
    <td class="tg-0pky">0.618</td>
    <td class="tg-0pky">0.105</td>
    <td class="tg-0pky">0.843</td>
    <td class="tg-0pky">0.082</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="4">NIMA</td>
    <td class="tg-0pky">nima</td>
    <td class="tg-0pky">4.941</td>
    <td class="tg-0pky">0.537</td>
    <td class="tg-0pky">7.056</td>
    <td class="tg-0pky">2.463</td>
  </tr>
  <tr>
    <td class="tg-0pky">nima-koniq</td>
    <td class="tg-0pky">0.654</td>
    <td class="tg-0pky">0.084</td>
    <td class="tg-0pky">0.849</td>
    <td class="tg-0pky">0.048</td>
  </tr>
  <tr>
    <td class="tg-0pky">nima-spaq</td>
    <td class="tg-0pky">71.036</td>
    <td class="tg-0pky">10.099</td>
    <td class="tg-0pky">98.191</td>
    <td class="tg-0pky">12.237</td>
  </tr>
  <tr>
    <td class="tg-0pky">nima-vgg-ava</td>
    <td class="tg-0pky">5.040</td>
    <td class="tg-0pky">0.503</td>
    <td class="tg-0pky">7.327</td>
    <td class="tg-0pky">2.374</td>
  </tr>
  <tr>
    <td class="tg-0pky">WaDIQaM</td>
    <td class="tg-0pky">wadiqam_nr</td>
    <td class="tg-0pky">-0.066</td>
    <td class="tg-0pky">0.207</td>
    <td class="tg-0pky">0.377</td>
    <td class="tg-0pky">-1.281</td>
  </tr>
  <tr>
    <td class="tg-0pky">CNNIQA</td>
    <td class="tg-0pky">cnniqa</td>
    <td class="tg-0pky">0.655</td>
    <td class="tg-0pky">0.070</td>
    <td class="tg-0pky">0.759</td>
    <td class="tg-0pky">0.089</td>
  </tr>
  <tr>
    <td class="tg-0pky">NRQM(Ma)^2^</td>
    <td class="tg-0pky">nrqm</td>
    <td class="tg-0pky">8.050</td>
    <td class="tg-0pky">1.001</td>
    <td class="tg-0pky">9.222</td>
    <td class="tg-0pky">1.600</td>
  </tr>
  <tr>
    <td class="tg-0pky">PI</td>
    <td class="tg-0pky">pi</td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
  </tr>
  <tr>
    <td class="tg-0pky">BRISQUE</td>
    <td class="tg-0pky">brisque</td>
    <td class="tg-0pky">13.777</td>
    <td class="tg-0pky">11.891</td>
    <td class="tg-0pky">184.089</td>
    <td class="tg-0pky">-67.742</td>
  </tr>
  <tr>
    <td class="tg-0pky">ILNIQE</td>
    <td class="tg-0pky">ilniqe</td>
    <td class="tg-0pky">22.919</td>
    <td class="tg-0pky">6.589</td>
    <td class="tg-0pky">154.256</td>
    <td class="tg-0pky">12.733</td>
  </tr>
  <tr>
    <td class="tg-0pky">NIQE</td>
    <td class="tg-0pky">niqe</td>
    <td class="tg-0pky">3.718</td>
    <td class="tg-0pky">1.082</td>
    <td class="tg-0pky">55.155</td>
    <td class="tg-0pky">1.430</td>
  </tr>
  <tr>
    <td class="tg-0pky">PIQE</td>
    <td class="tg-0pky">piqe</td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
  </tr>
</tbody></table>
</section>
</section>
<section id="id7">
<h3>针对生成图像的评估指标<a class="headerlink" href="#id7" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>指标</p></th>
<th class="head"><p>名称（用于<code class="docutils literal notranslate"><span class="pre">datagym.get_scorer()</span></code>）</p></th>
<th class="head"><p>评估维度</p></th>
<th class="head"><p>简介</p></th>
<th class="head"><p>取值范围</p></th>
<th class="head"><p>官方仓库或论文</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FID</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">fid_score</span></code></p></td>
<td><p>生成图像与真实图像间的统计差异</p></td>
<td><p>使用Inception网络计算特征，进而计算两个数据集的统计距离，评估生成模型的质量。</p></td>
<td><p>最佳值为0，较低的值表明较小的差异和更高的图像质量，无上限</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1706.08500">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>KID</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">kid_score</span></code></p></td>
<td><p>生成图像的无偏质量估计</p></td>
<td><p>Kernel Inception Distance，使用Inception网络特征计算MMD，提供对生成图像质量的无偏估计。</p></td>
<td><p>最佳值为0，较低的值表示更低的偏差和更好的图像质量，无上限</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1801.01401">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>IS</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">is_score</span></code></p></td>
<td><p>生成图像的多样性和清晰度</p></td>
<td><p>通过计算生成图像的Inception网络输出的信息熵，评估图像多样性及清晰度。</p></td>
<td><p>值越高表示图像质量越好，通常分数在1到10之间，但无具体上限</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1606.03498">paper</a></p></td>
</tr>
</tbody>
</table>
<section id="id8">
<h4>参考值<a class="headerlink" href="#id8" title="Link to this heading"></a></h4>
<p>为更好的提供数据质量参考，我们使用了四种模型：flux-dev, flux-schnell, stable-diffusion-3-medium 和 sdxl，对在 LLaVA Pretrain 数据集上随机选取的500个 image-caption 对进行测试。每个模型根据给定的 caption 生成相应的图片，并通过上述三个指标对生成图片的质量进行全面评估。结果如下：</p>
<table class="tg"><thead>
  <tr>
    <th class="tg-0pky">模型名称</th>
    <th class="tg-0pky">Inception Score (IS)</th>
    <th class="tg-0pky">Fréchet Inception Distance (FID)</th>
    <th class="tg-0pky">Kernel Inception Distance (KID)</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-0pky">flux-dev</td>
    <td class="tg-0pky">7.195 ± 0.809</td>
    <td class="tg-0pky">101.572</td>
    <td class="tg-0pky">0.00903 ± 0.00069</td>
  </tr>
  <tr>
    <td class="tg-0pky">flux-schnell</td>
    <td class="tg-0pky">6.193 ± 0.546</td>
    <td class="tg-0pky">102.739</td>
    <td class="tg-0pky">0.00667 ± 0.00055</td>
  </tr>
  <tr>
    <td class="tg-0pky">stable-diffusion-3-medium</td>
    <td class="tg-0pky">6.740 ± 0.582</td>
    <td class="tg-0pky">100.235</td>
    <td class="tg-0pky">0.00609 ± 0.00056</td>
  </tr>
  <tr>
    <td class="tg-0pky">sdxl</td>
    <td class="tg-0pky">6.809 ± 0.994</td>
    <td class="tg-0pky">112.807</td>
    <td class="tg-0pky">0.01051 ± 0.00065</td>
  </tr>
</tbody></table>
stable-diffusion-3-medium在FID指标上表现最佳，表明其生成的图像与真实图像在统计特征上最为接近。而在IS评分中，flux-dev表现最优，显示了较高的图像多样性和清晰度。KID结果中，stable-diffusion-3-medium同样表现较好，表示其图像质量的偏差较小。
</section>
</section>
</section>
<section id="id9">
<h2>图像-文本评估指标<a class="headerlink" href="#id9" title="Link to this heading"></a></h2>
<section id="id10">
<h3>图文对齐指标<a class="headerlink" href="#id10" title="Link to this heading"></a></h3>
<p>指标数值越高，则image-caption对齐程度越好。</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>指标</p></th>
<th class="head"><p>名称（用于<code class="docutils literal notranslate"><span class="pre">datagym.get_scorer()</span></code></p></th>
<th class="head"><p>数据类型</p></th>
<th class="head"><p>简介</p></th>
<th class="head"><p>取值范围</p></th>
<th class="head"><p>官方仓库或论文</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CLIP</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">clip</span></code></p></td>
<td><p>image-caption</p></td>
<td><p>经典的图文对齐分数。数值越大，对齐程度越高。</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://github.com/openai/CLIP">code</a></p></td>
</tr>
<tr class="row-odd"><td><p>LongCLIP</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">longclip</span></code></p></td>
<td><p>image-caption</p></td>
<td><p>可输入更长文本、粒度更细的CLIP。数值越大，对齐程度越高。</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://github.com/beichenzbc/Long-CLIP">code</a></p></td>
</tr>
<tr class="row-even"><td><p>FLEUR</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">fleur</span></code></p></td>
<td><p>image-caption</p></td>
<td><p>使用LLaVA模型进行评分。数值越大，对齐程度越高。</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://github.com/Yebin46/FLEUR">code</a></p></td>
</tr>
<tr class="row-odd"><td><p>VQA Score</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">vqa_score</span></code></p></td>
<td><p>image-caption</p></td>
<td><p>使用CLIP-FlanT5模型进行打分。数值越大，对齐程度越高。</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://github.com/linzhiqiu/t2v_metrics">code</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="sft">
<h3>图文SFT数据评估指标<a class="headerlink" href="#sft" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>指标名称</p></th>
<th class="head"><p>评估维度</p></th>
<th class="head"><p>数据类型</p></th>
<th class="head"><p>简介</p></th>
<th class="head"><p>取值范围</p></th>
<th class="head"><p>官方仓库或论文</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>vqa_score</p></td>
<td><p>图片-对话对齐度</p></td>
<td><p>image-dialog</p></td>
<td><p>使用LLaVA模型判断对话正误。数值越大，对齐程度越高。</p></td>
<td><p>(-∞,0]</p></td>
<td><p>/</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="页脚">
        <a href="image_metrics.html" class="btn btn-neutral float-left" title="Image Data Evaluation Metrics" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="text_metrics.html" class="btn btn-neutral float-right" title="Text Data Evaluation Metrics" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2024, Open-DataFlow。</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>