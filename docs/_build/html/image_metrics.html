<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Image Data Evaluation Metrics &mdash; Open-DataFlow-Eval 0.1 文档</title>
      <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script src="_static/jquery.js?v=5d32c60e"></script>
        <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
        <script src="_static/documentation_options.js?v=7c91f8fd"></script>
        <script src="_static/doctools.js?v=9a2dae69"></script>
        <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
        <script src="_static/translations.js?v=beaddf03"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="索引" href="genindex.html" />
    <link rel="search" title="搜索" href="search.html" />
    <link rel="next" title="Video Data Evaluation Metrics" href="video_metrics.html" />
    <link rel="prev" title="Text Data Evaluation Metrics" href="text_metrics.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            Open-DataFlow-Eval
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="text_metrics.html">Text Data Evaluation Metrics</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Image Data Evaluation Metrics</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#pure-image-evaluation-metrics">Pure Image Evaluation Metrics</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#metric-classification">Metric Classification</a></li>
<li class="toctree-l3"><a class="reference internal" href="#evaluation-metrics-for-real-images">Evaluation Metrics for Real Images</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#metric-introduction">Metric Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#reference-values">Reference Values</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#evaluation-metrics-for-generated-images">Evaluation Metrics for Generated Images</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">Reference Values</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#image-text-evaluation-metrics">Image-Text Evaluation Metrics</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#image-text-alignment-metrics">Image-Text Alignment Metrics</a></li>
<li class="toctree-l4"><a class="reference internal" href="#sft-data-evaluation-metrics-for-image-text">SFT Data Evaluation Metrics for Image-Text</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="video_metrics.html">Video Data Evaluation Metrics</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Open-DataFlow-Eval</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Image Data Evaluation Metrics</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/image_metrics.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="image-data-evaluation-metrics">
<h1>Image Data Evaluation Metrics<a class="headerlink" href="#image-data-evaluation-metrics" title="Link to this heading"></a></h1>
<!-- Use `dataflow.list_image_eval_metrics()` to print all available image evaluation metrics.
```python
import dataflow
dataflow.list_image_eval_metrics()
``` -->
<section id="pure-image-evaluation-metrics">
<h2>Pure Image Evaluation Metrics<a class="headerlink" href="#pure-image-evaluation-metrics" title="Link to this heading"></a></h2>
<section id="metric-classification">
<h3>Metric Classification<a class="headerlink" href="#metric-classification" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Category Description</p></th>
<th class="head"><p>Metric List</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Based on Image Statistics</p></td>
<td><p>BRISQUE, ILNIQE, NIQE, PIQE, FID, KID, IS</p></td>
</tr>
<tr class="row-odd"><td><p>Based on Neural Networks</p></td>
<td><p>ARNIQA, TOPIQ, TReS, MANIQA, MUSIQ, DBCNN, PaQ-2-PiQ, HyperIQA, NIMA, WaDIQaM, CNNIQA</p></td>
</tr>
<tr class="row-even"><td><p>Based on Pre-trained Image-Text Models</p></td>
<td><p>Q-Align, CLIPIQA(+), LIQE</p></td>
</tr>
</tbody>
</table>
</section>
<section id="evaluation-metrics-for-real-images">
<h3>Evaluation Metrics for Real Images<a class="headerlink" href="#evaluation-metrics-for-real-images" title="Link to this heading"></a></h3>
<section id="metric-introduction">
<h4>Metric Introduction<a class="headerlink" href="#metric-introduction" title="Link to this heading"></a></h4>
<p>This repository uses non-reference (NR) algorithms from the <a class="reference external" href="https://github.com/chaofengc/IQA-PyTorch">pyiqa</a> package for pure image data quality assessment. Introductions to each evaluation metric can be found in the <a class="reference external" href="https://github.com/chaofengc/IQA-PyTorch/blob/main/docs/ModelCard.md">Py-IQA Model Card</a>.</p>
<p>Note: When the same metric uses different training datasets, we distinguish them using <code class="docutils literal notranslate"><span class="pre">Metric</span> <span class="pre">Name-Dataset</span> <span class="pre">Name</span></code>. For example, <code class="docutils literal notranslate"><span class="pre">arniqa-csiq</span></code> uses <code class="docutils literal notranslate"><span class="pre">csiq</span></code> as the dataset name. When the dataset name is not specified, it defaults to <code class="docutils literal notranslate"><span class="pre">koniq</span></code>, such as <code class="docutils literal notranslate"><span class="pre">arniqa</span></code> which corresponds to the <code class="docutils literal notranslate"><span class="pre">koniq</span></code> dataset.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Name (for <code class="docutils literal notranslate"><span class="pre">datagym.get_scorer()</span></code>)</p></th>
<th class="head"><p>Evaluation Dimension</p></th>
<th class="head"><p>Introduction</p></th>
<th class="head"><p>Value Range</p></th>
<th class="head"><p>Official Repository or Paper</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Q-Align</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">qalign</span></code> (with quality[default], aesthetic options)</p></td>
<td><p>Based on Pre-trained Image-Text Model</p></td>
<td><p>Scoring using Visual LLM. The larger the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p>[1,5]</p></td>
</tr>
<tr class="row-odd"><td><p>LIQE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">liqe</span></code>, <code class="docutils literal notranslate"><span class="pre">liqe_mix</span></code></p></td>
<td><p>Based on Pre-trained Image-Text Model</p></td>
<td><p>Based on CLIP. The larger the value, the higher the quality.</p></td>
<td><p>[1,5]</p></td>
<td><p><a class="reference external" href="https://github.com/zwx8981/LIQE">code</a></p></td>
</tr>
<tr class="row-even"><td><p>ARNIQA</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">arniqa</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-live</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-csiq</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-tid</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-kadid</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-clive</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-flive</span></code>, <code class="docutils literal notranslate"><span class="pre">arniqa-spaq</span></code></p></td>
<td><p>Based on Neural Networks</p></td>
<td><p>Learning the Image Distortion Manifold. The larger the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2310.14918">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>TOPIQ</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">topiq_nr</span></code>, <code class="docutils literal notranslate"><span class="pre">topiq_nr-flive</span></code>, <code class="docutils literal notranslate"><span class="pre">topiq_nr-spaq</span></code></p></td>
<td><p>Based on Neural Networks</p></td>
<td><p>Semantic-based Top-down Image Quality Assessment. The larger the value, the higher the quality.</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2308.03060">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>TReS</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">tres</span></code>, <code class="docutils literal notranslate"><span class="pre">tres-flive</span></code></p></td>
<td><p>Based on Neural Networks</p></td>
<td><p>Enhancing Metric Robustness through Relative Ranking and Self-consistency. The larger the value, the higher the quality.</p></td>
<td><p>[0,100]</p></td>
<td><p><a class="reference external" href="https://github.com/isalirezag/TReS">code</a></p></td>
</tr>
<tr class="row-odd"><td><p>CLIPIQA(+)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">clipiqa</span></code>, <code class="docutils literal notranslate"><span class="pre">clipiqa+</span></code>, <code class="docutils literal notranslate"><span class="pre">clipiqa+_vitL14_512</span></code>, <code class="docutils literal notranslate"><span class="pre">clipiqa+_rn50_512</span></code></p></td>
<td><p>Based on Pre-trained Image-Text Model</p></td>
<td><p>Based on CLIP with Antonym prompt pairing. CLIPIQA(+) with different backbone networks, default is RN50. The larger the value, the higher the quality.</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://github.com/IceClear/CLIP-IQA">code</a></p></td>
</tr>
<tr class="row-even"><td><p>MANIQA</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">maniqa</span></code>, <code class="docutils literal notranslate"><span class="pre">maniqa-kadid</span></code>, <code class="docutils literal notranslate"><span class="pre">maniqa-pipal</span></code></p></td>
<td><p>Based on Neural Networks</p></td>
<td><p>Designed a Multi-dimension Attention Network for Quality Assessment. The larger the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2204.08958">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>MUSIQ</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">musiq</span></code>, <code class="docutils literal notranslate"><span class="pre">musiq-spaq</span></code>, <code class="docutils literal notranslate"><span class="pre">musiq-paq2piq</span></code>, <code class="docutils literal notranslate"><span class="pre">musiq-ava</span></code></p></td>
<td><p>Based on Neural Networks</p></td>
<td><p>Designed a Multi-scale Image Quality Assessment Transformer. The larger the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/2108.05997">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>DBCNN</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">dbcnn</span></code></p></td>
<td><p>Based on Neural Networks</p></td>
<td><p>Designed a Bilinear Model to Address Synthetic and Realistic Distortions. The larger the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/document/8576582">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>PaQ-2-PiQ</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">paq2piq</span></code></p></td>
<td><p>Based on Neural Networks</p></td>
<td><p>Designed a Quality Assessment Structure that Generates Global-to-Local and Local-to-Global Inferences. The larger the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://baidut.github.io/PaQ-2-PiQ/">code</a></p></td>
</tr>
<tr class="row-even"><td><p>HyperIQA</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">hyperiqa</span></code></p></td>
<td><p>Based on Neural Networks</p></td>
<td><p>Designed an Adaptive Hypernetwork Architecture to Handle Realistic Image Distortions. The larger the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Su_Blindly_Assess_Image_Quality_in_the_Wild_Guided_by_a_CVPR_2020_paper.pdf">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>NIMA</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nima</span></code>, <code class="docutils literal notranslate"><span class="pre">nima-vgg16-ava</span></code></p></td>
<td><p>Based on Neural Networks</p></td>
<td><p>Predicting Human Opinion Scores using Convolutional Neural Networks <strong>Distribution</strong>. The larger the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1709.05424">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>WaDIQaM</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">wadiqam_nr</span></code></p></td>
<td><p>Based on Neural Networks</p></td>
<td><p>Based on Convolutional Neural Networks. The larger the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/abstract/document/8063957">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>CNNIQA</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">cnniqa</span></code></p></td>
<td><p>Based on Neural Networks</p></td>
<td><p>Based on Convolutional Neural Networks. The larger the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2014/papers/Kang_Convolutional_Neural_Networks_2014_CVPR_paper.pdf">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>NRQM(Ma)^2^</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">nrqm</span></code></p></td>
<td><p>Super-Resolution Image Assessment</p></td>
<td><p>Based on Image Statistics. The smaller the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1612.05890">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>PI (Perceptual Index)</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">pi</span></code></p></td>
<td><p>Super-Resolution Image Assessment</p></td>
<td><p>Based on Ma’s score and NIQE. The smaller the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1711.06077">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>BRISQUE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">brisque</span></code>, <code class="docutils literal notranslate"><span class="pre">brisque_matlab</span></code></p></td>
<td><p>Based on Image Statistics</p></td>
<td><p>Assessed in the Spatial Domain; Low Computational Complexity. The smaller the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/document/6272356">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>ILNIQE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">ilniqe</span></code></p></td>
<td><p>Based on Image Statistics</p></td>
<td><p>Based on Natural Image Statistical Features. The smaller the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/document/7094273">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>NIQE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">niqe</span></code>, <code class="docutils literal notranslate"><span class="pre">niqe_matlab</span></code></p></td>
<td><p>Based on Image Statistics</p></td>
<td><p>Based on Statistical Features of Natural, Undistorted Image Data. The smaller the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/document/6353522">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>PIQE</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">piqe</span></code></p></td>
<td><p>Based on Image Statistics</p></td>
<td><p>Extract Local Features to Predict Quality; Low Computational Complexity. The smaller the value, the higher the quality.</p></td>
<td><p></p></td>
<td><p><a class="reference external" href="https://ieeexplore.ieee.org/document/7084843">paper</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="reference-values">
<h4>Reference Values<a class="headerlink" href="#reference-values" title="Link to this heading"></a></h4>
<p>To better provide data quality references, we have evaluated the MSCOCO 2017 train using the above metrics, and the distribution of metric values obtained is as follows:</p>
<table class="tg"><thead>
  <tr>
    <th class="tg-0pky">Metric</th>
    <th class="tg-0pky">Name</th>
    <th class="tg-0pky">Mean</th>
    <th class="tg-0pky">Variance</th>
    <th class="tg-0pky">Maximum</th>
    <th class="tg-0pky">Minimum</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-0pky">Q-Align</td>
    <td class="tg-0pky">qalign</td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="2">LIQE</td>
    <td class="tg-0pky">liqe</td>
    <td class="tg-0pky">4.152</td>
    <td class="tg-0pky">1.004</td>
    <td class="tg-0pky">5.000</td>
    <td class="tg-0pky">1.000</td>
  </tr>
  <tr>
    <td class="tg-0pky">liqe_mix</td>
    <td class="tg-0pky">4.090</td>
    <td class="tg-0pky">0.893</td>
    <td class="tg-0pky">5.000</td>
    <td class="tg-0pky">1.000</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="9">ARNIQA</td>
    <td class="tg-0pky">arniqa</td>
    <td class="tg-0pky">0.705</td>
    <td class="tg-0pky">0.069</td>
    <td class="tg-0pky">0.867</td>
    <td class="tg-0pky">0.150</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-clive</td>
    <td class="tg-0pky">0.649</td>
    <td class="tg-0pky">0.103</td>
    <td class="tg-0pky">0.961</td>
    <td class="tg-0pky">-0.105</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-csiq</td>
    <td class="tg-0pky">0.900</td>
    <td class="tg-0pky">0.073</td>
    <td class="tg-0pky">1.081</td>
    <td class="tg-0pky">0.319</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-flive</td>
    <td class="tg-0pky">0.724</td>
    <td class="tg-0pky">0.036</td>
    <td class="tg-0pky">0.838</td>
    <td class="tg-0pky">0.097</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-kadid</td>
    <td class="tg-0pky">0.635</td>
    <td class="tg-0pky">0.122</td>
    <td class="tg-0pky">0.965</td>
    <td class="tg-0pky">-0.013</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-koniq</td>
    <td class="tg-0pky">0.705</td>
    <td class="tg-0pky">0.069</td>
    <td class="tg-0pky">0.867</td>
    <td class="tg-0pky">0.150</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-live</td>
    <td class="tg-0pky">0.788</td>
    <td class="tg-0pky">0.069</td>
    <td class="tg-0pky">0.958</td>
    <td class="tg-0pky">0.227</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-spqa</td>
    <td class="tg-0pky">0.699</td>
    <td class="tg-0pky">0.104</td>
    <td class="tg-0pky">1.100</td>
    <td class="tg-0pky">0.056</td>
  </tr>
  <tr>
    <td class="tg-0pky">arniqa-tid</td>
    <td class="tg-0pky">0.548</td>
    <td class="tg-0pky">0.081</td>
    <td class="tg-0pky">0.803</td>
    <td class="tg-0pky">0.140</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="5">TOPIQ</td>
    <td class="tg-0pky">topiq_nr</td>
    <td class="tg-0pky">0.610</td>
    <td class="tg-0pky">0.116</td>
    <td class="tg-0pky">0.851</td>
    <td class="tg-0pky">0.073</td>
  </tr>
  <tr>
    <td class="tg-0pky">topiq_iaa_res50</td>
    <td class="tg-0pky">5.013</td>
    <td class="tg-0pky">0.492</td>
    <td class="tg-0pky">6.969</td>
    <td class="tg-0pky">2.812</td>
  </tr>
  <tr>
    <td class="tg-0pky">topiq_iaa</td>
    <td class="tg-0pky">4.838</td>
    <td class="tg-0pky">0.539</td>
    <td class="tg-0pky">7.129</td>
    <td class="tg-0pky">2.607</td>
  </tr>
  <tr>
    <td class="tg-0pky">topiq_nr-flive</td>
    <td class="tg-0pky">0.728</td>
    <td class="tg-0pky">0.036</td>
    <td class="tg-0pky">0.825</td>
    <td class="tg-0pky">0.371</td>
  </tr>
  <tr>
    <td class="tg-0pky">topiq_nr-spaq</td>
    <td class="tg-0pky">0.679</td>
    <td class="tg-0pky">0.102</td>
    <td class="tg-0pky">0.930</td>
    <td class="tg-0pky">0.119</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="4">CLIPIQA(+)</td>
    <td class="tg-0pky">clipiqa</td>
    <td class="tg-0pky">0.622</td>
    <td class="tg-0pky">0.149</td>
    <td class="tg-0pky">0.934</td>
    <td class="tg-0pky">0.056</td>
  </tr>
  <tr>
    <td class="tg-0pky">clipiqa+</td>
    <td class="tg-0pky">0.659</td>
    <td class="tg-0pky">0.100</td>
    <td class="tg-0pky">0.918</td>
    <td class="tg-0pky">0.130</td>
  </tr>
  <tr>
    <td class="tg-0pky">clipiqa+_rn50_512</td>
    <td class="tg-0pky">0.571</td>
    <td class="tg-0pky">0.122</td>
    <td class="tg-0pky">0.883</td>
    <td class="tg-0pky">0.050</td>
  </tr>
  <tr>
    <td class="tg-0pky">clipiqa+_vitL14_512</td>
    <td class="tg-0pky">0.593</td>
    <td class="tg-0pky">0.128</td>
    <td class="tg-0pky">0.893</td>
    <td class="tg-0pky">0.077</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="4">MANIQA</td>
    <td class="tg-0pky">maniqa</td>
    <td class="tg-0pky">0.454</td>
    <td class="tg-0pky">0.106</td>
    <td class="tg-0pky">0.789</td>
    <td class="tg-0pky">0.021</td>
  </tr>
  <tr>
    <td class="tg-0pky">maniqa-kadid</td>
    <td class="tg-0pky">0.637</td>
    <td class="tg-0pky">0.122</td>
    <td class="tg-0pky">0.877</td>
    <td class="tg-0pky">0.075</td>
  </tr>
  <tr>
    <td class="tg-0pky">maniqa-koniq</td>
    <td class="tg-0pky">0.454</td>
    <td class="tg-0pky">0.106</td>
    <td class="tg-0pky">0.789</td>
    <td class="tg-0pky">0.021</td>
  </tr>
  <tr>
    <td class="tg-0pky">maniqa-pipal</td>
    <td class="tg-0pky">0.676</td>
    <td class="tg-0pky">0.062</td>
    <td class="tg-0pky">0.888</td>
    <td class="tg-0pky">0.228</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="5">MUSIQ</td>
    <td class="tg-0pky">musiq</td>
    <td class="tg-0pky">69.086</td>
    <td class="tg-0pky">7.833</td>
    <td class="tg-0pky">79.559</td>
    <td class="tg-0pky">12.679</td>
  </tr>
  <tr>
    <td class="tg-0pky">musiq-ava</td>
    <td class="tg-0pky">4.939</td>
    <td class="tg-0pky">0.546</td>
    <td class="tg-0pky">7.269</td>
    <td class="tg-0pky">2.434</td>
  </tr>
  <tr>
    <td class="tg-0pky">musiq-koniq</td>
    <td class="tg-0pky">69.086</td>
    <td class="tg-0pky">7.833</td>
    <td class="tg-0pky">79.559</td>
    <td class="tg-0pky">12.679</td>
  </tr>
  <tr>
    <td class="tg-0pky">musiq-paq2piq</td>
    <td class="tg-0pky">72.792</td>
    <td class="tg-0pky">3.520</td>
    <td class="tg-0pky">79.772</td>
    <td class="tg-0pky">39.551</td>
  </tr>
  <tr>
    <td class="tg-0pky">musiq-spaq</td>
    <td class="tg-0pky">70.534</td>
    <td class="tg-0pky">8.661</td>
    <td class="tg-0pky">81.385</td>
    <td class="tg-0pky">14.290</td>
  </tr>
  <tr>
    <td class="tg-0pky">DBCNN</td>
    <td class="tg-0pky">dbcnn</td>
    <td class="tg-0pky">0.634</td>
    <td class="tg-0pky">0.100</td>
    <td class="tg-0pky">0.834</td>
    <td class="tg-0pky">0.143</td>
  </tr>
  <tr>
    <td class="tg-0pky">PaQ-2-PiQ</td>
    <td class="tg-0pky">paq2piq</td>
    <td class="tg-0pky">74.669</td>
    <td class="tg-0pky">3.731</td>
    <td class="tg-0pky">85.906</td>
    <td class="tg-0pky">15.859</td>
  </tr>
  <tr>
    <td class="tg-0pky">HyperIQA</td>
    <td class="tg-0pky">hyperiqa</td>
    <td class="tg-0pky">0.618</td>
    <td class="tg-0pky">0.105</td>
    <td class="tg-0pky">0.843</td>
    <td class="tg-0pky">0.082</td>
  </tr>
  <tr>
    <td class="tg-0pky" rowspan="4">NIMA</td>
    <td class="tg-0pky">nima</td>
    <td class="tg-0pky">4.941</td>
    <td class="tg-0pky">0.537</td>
    <td class="tg-0pky">7.056</td>
    <td class="tg-0pky">2.463</td>
  </tr>
  <tr>
    <td class="tg-0pky">nima-koniq</td>
    <td class="tg-0pky">0.654</td>
    <td class="tg-0pky">0.084</td>
    <td class="tg-0pky">0.849</td>
    <td class="tg-0pky">0.048</td>
  </tr>
  <tr>
    <td class="tg-0pky">nima-spaq</td>
    <td class="tg-0pky">71.036</td>
    <td class="tg-0pky">10.099</td>
    <td class="tg-0pky">98.191</td>
    <td class="tg-0pky">12.237</td>
  </tr>
  <tr>
    <td class="tg-0pky">nima-vgg-ava</td>
    <td class="tg-0pky">5.040</td>
    <td class="tg-0pky">0.503</td>
    <td class="tg-0pky">7.327</td>
    <td class="tg-0pky">2.374</td>
  </tr>
  <tr>
    <td class="tg-0pky">WaDIQaM</td>
    <td class="tg-0pky">wadiqam_nr</td>
    <td class="tg-0pky">-0.066</td>
    <td class="tg-0pky">0.207</td>
    <td class="tg-0pky">0.377</td>
    <td class="tg-0pky">-1.281</td>
  </tr>
  <tr>
    <td class="tg-0pky">CNNIQA</td>
    <td class="tg-0pky">cnniqa</td>
    <td class="tg-0pky">0.655</td>
    <td class="tg-0pky">0.070</td>
    <td class="tg-0pky">0.759</td>
    <td class="tg-0pky">0.089</td>
  </tr>
  <tr>
    <td class="tg-0pky">NRQM(Ma)^2^</td>
    <td class="tg-0pky">nrqm</td>
    <td class="tg-0pky">8.050</td>
    <td class="tg-0pky">1.001</td>
    <td class="tg-0pky">9.222</td>
    <td class="tg-0pky">1.600</td>
  </tr>
  <tr>
    <td class="tg-0pky">PI</td>
    <td class="tg-0pky">pi</td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
  </tr>
  <tr>
    <td class="tg-0pky">BRISQUE</td>
    <td class="tg-0pky">brisque</td>
    <td class="tg-0pky">13.777</td>
    <td class="tg-0pky">11.891</td>
    <td class="tg-0pky">184.089</td>
    <td class="tg-0pky">-67.742</td>
  </tr>
  <tr>
    <td class="tg-0pky">ILNIQE</td>
    <td class="tg-0pky">ilniqe</td>
    <td class="tg-0pky">22.919</td>
    <td class="tg-0pky">6.589</td>
    <td class="tg-0pky">154.256</td>
    <td class="tg-0pky">12.733</td>
  </tr>
  <tr>
    <td class="tg-0pky">NIQE</td>
    <td class="tg-0pky">niqe</td>
    <td class="tg-0pky">3.718</td>
    <td class="tg-0pky">1.082</td>
    <td class="tg-0pky">55.155</td>
    <td class="tg-0pky">1.430</td>
  </tr>
  <tr>
    <td class="tg-0pky">PIQE</td>
    <td class="tg-0pky">piqe</td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
    <td class="tg-0pky"></td>
  </tr>
</tbody></table>
</section>
</section>
<section id="evaluation-metrics-for-generated-images">
<h3>Evaluation Metrics for Generated Images<a class="headerlink" href="#evaluation-metrics-for-generated-images" title="Link to this heading"></a></h3>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Name for <code class="docutils literal notranslate"><span class="pre">datagym.get_scorer()</span></code></p></th>
<th class="head"><p>Evaluation Dimension</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Range</p></th>
<th class="head"><p>Official Repository or Paper</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>FID</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">fid_score</span></code></p></td>
<td><p>Statistical difference between generated and real images</p></td>
<td><p>Uses Inception network to calculate features and then the statistical distance between two datasets to evaluate the quality of generative models.</p></td>
<td><p>Best value is 0, lower values indicate smaller differences and higher image quality, no upper limit</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1706.08500">paper</a></p></td>
</tr>
<tr class="row-odd"><td><p>KID</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">kid_score</span></code></p></td>
<td><p>Unbiased quality estimation of generated images</p></td>
<td><p>Kernel Inception Distance, uses Inception network features to calculate MMD, providing an unbiased estimation of the quality of generated images.</p></td>
<td><p>Best value is 0, lower values indicate lower bias and better image quality, no upper limit</p></td>
<td><p><a class="reference external" href="https://arxiv.org/abs/1801.01401">paper</a></p></td>
</tr>
<tr class="row-even"><td><p>IS</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">is_score</span></code></p></td>
<td><p>Diversity and clarity of generated images</p></td>
<td><p>Evaluates the diversity and clarity of images by calculating the entropy of the Inception network’s output.</p></td>
<td><p>Higher values indicate better image quality, typically scores range from 1 to 10, but no specific upper limit</p></td>
<td><p><a class="reference external" href="https://arxiv.org/pdf/1606.03498">paper</a></p></td>
</tr>
</tbody>
</table>
<section id="id1">
<h4>Reference Values<a class="headerlink" href="#id1" title="Link to this heading"></a></h4>
<p>To better provide data quality references, we used four models: flux-dev, flux-schnell, stable-diffusion-3-medium, and sdxl, to test 500 randomly selected image-caption pairs from the LLaVA Pretrain dataset. Each model generated images based on the given captions, and their quality was comprehensively assessed using the following three indicators. Here are the results:</p>
<table class="tg"><thead>
  <tr>
    <th class="tg-0pky">Model Name</th>
    <th class="tg-0pky">Inception Score (IS)</th>
    <th class="tg-0pky">Fréchet Inception Distance (FID)</th>
    <th class="tg-0pky">Kernel Inception Distance (KID)</th>
  </tr></thead>
<tbody>
  <tr>
    <td class="tg-0pky">flux-dev</td>
    <td class="tg-0pky">7.195 ± 0.809</td>
    <td class="tg-0pky">101.572</td>
    <td class="tg-0pky">0.00903 ± 0.00069</td>
  </tr>
  <tr>
    <td class="tg-0pky">flux-schnell</td>
    <td class="tg-0pky">6.193 ± 0.546</td>
    <td class="tg-0pky">102.739</td>
    <td class="tg-0pky">0.00667 ± 0.00055</td>
  </tr>
  <tr>
    <td class="tg-0pky">stable-diffusion-3-medium</td>
    <td class="tg-0pky">6.740 ± 0.582</td>
    <td class="tg-0pky">100.235</td>
    <td class="tg-0pky">0.00609 ± 0.00056</td>
  </tr>
  <tr>
    <td class="tg-0pky">sdxl</td>
    <td class="tg-0pky">6.809 ± 0.994</td>
    <td class="tg-0pky">112.807</td>
    <td class="tg-0pky">0.01051 ± 0.00065</td>
  </tr>
</tbody></table>
Stable-diffusion-3-medium performed the best in terms of FID, indicating that its generated images are statistically closest to real images. Flux-dev showed the best results in the IS score, reflecting higher diversity and clarity in images. Similarly, stable-diffusion-3-medium also exhibited superior performance in KID results, indicating a smaller deviation in image quality.
</section>
</section>
<section id="image-text-evaluation-metrics">
<h3>Image-Text Evaluation Metrics<a class="headerlink" href="#image-text-evaluation-metrics" title="Link to this heading"></a></h3>
<section id="image-text-alignment-metrics">
<h4>Image-Text Alignment Metrics<a class="headerlink" href="#image-text-alignment-metrics" title="Link to this heading"></a></h4>
<p>Higher metric values indicate better alignment between images and captions.</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Name for <code class="docutils literal notranslate"><span class="pre">datagym.get_scorer()</span></code></p></th>
<th class="head"><p>Data Type</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Range</p></th>
<th class="head"><p>Official Repository or Paper</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>CLIP</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">clip</span></code></p></td>
<td><p>image-caption</p></td>
<td><p>Classic image-text alignment score. The larger the value, the higher the alignment degree.</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://github.com/openai/CLIP">code</a></p></td>
</tr>
<tr class="row-odd"><td><p>LongCLIP</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">longclip</span></code></p></td>
<td><p>image-caption</p></td>
<td><p>CLIP with longer text input and finer granularity. The larger the value, the higher the alignment degree.</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://github.com/beichenzbc/Long-CLIP">code</a></p></td>
</tr>
<tr class="row-even"><td><p>FLEUR</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">fleur</span></code></p></td>
<td><p>image-caption</p></td>
<td><p>Scores using the LLaVA model. The larger the value, the higher the alignment degree.</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://github.com/Yebin46/FLEUR">code</a></p></td>
</tr>
<tr class="row-odd"><td><p>VQA Score</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">vqa_score</span></code></p></td>
<td><p>image-caption</p></td>
<td><p>Scores using the CLIP-FlanT5 model. The larger the value, the higher the alignment degree.</p></td>
<td><p>[0,1]</p></td>
<td><p><a class="reference external" href="https://github.com/linzhiqiu/t2v_metrics">code</a></p></td>
</tr>
</tbody>
</table>
</section>
<section id="sft-data-evaluation-metrics-for-image-text">
<h4>SFT Data Evaluation Metrics for Image-Text<a class="headerlink" href="#sft-data-evaluation-metrics-for-image-text" title="Link to this heading"></a></h4>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Metric Name</p></th>
<th class="head"><p>Evaluation Dimension</p></th>
<th class="head"><p>Data Type</p></th>
<th class="head"><p>Description</p></th>
<th class="head"><p>Range</p></th>
<th class="head"><p>Official Repository or Paper</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>visual_dialog_score</p></td>
<td><p>Image-dialog alignment</p></td>
<td><p>image-dialog</p></td>
<td><p>Use the LLaVA model to judge the correctness of the dialogue. The larger the value, the higher the alignment degree.</p></td>
<td><p>(-∞,0]</p></td>
<td><p>/</p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="页脚">
        <a href="text_metrics.html" class="btn btn-neutral float-left" title="Text Data Evaluation Metrics" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="video_metrics.html" class="btn btn-neutral float-right" title="Video Data Evaluation Metrics" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2024, Open-DataFlow。</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>